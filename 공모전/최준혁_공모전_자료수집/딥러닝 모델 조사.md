## 예측(회귀)와 분류
![](https://i.imgur.com/DZIOQ0D.png)


### 예측(회귀) : Regression
정의 : **연속형 목표 변수**를 예측하는 방법

연속형 목표 변수 : 연속적인 숫자 값을 가지는 변수, 특정 범위 내에서 무한히 많은 값을 가질 수 있다.
* 주택 가격 예측 : 주택의 크기, 위치, 방 수 등의 특성을 입력 변수로 사용하여 주택의 가격을 예측 (예: $250,000, $300,000)
* 온도 예측: 과거 기온 데이터, 습도 등의 특성을 사용하여 특정 날짜의 기온을 예측 (예: 23.5°C, 30.2°C)

<br>


### 분류 : Classification
정의 : **이산형 목표 변수**를 분류하는 방법

이산형 목표 변수 : 특정한 이산적인 값(혹은 범주)을 가지는 변수, 고정된 값만을 가진다. (동전의 앞면은 1, 뒷면은 0같은)
* 스팸 이메일 분류 : 이메일의 텍스트 내용, 발신자 정보 등을 사용하여 이메일이 스팸인지 아닌지를 분류 (0: 스팸 아님, 1: 스팸)
* 질병 진단 : 환자의 증상, 의료 기록 등을 사용하여 특정 질병의 유무를 예측 (0: 질병 없음, 1: 질병 있음)


<br>

## 공통된 딥러닝 모델들
이 모델들은 예측(회귀)와 분류 문제 모두에 사용될 수 있으며, 각 문제에 특화된 변형이 존재한다.
* 다층 퍼셉트론(MLP)
* 순환 신경망(RNN)
* 컨볼루션 신경망(CNN)
* 트랜스포머(Transformer)

<br>



### 퍼셉트론 (Perceptron)

* 퍼셉트론의 모델이 된 뉴런(Neuron)

![](https://i.imgur.com/7DvsXog.png)



* 프랭크 로젠블릿의 인간의 뉴런을 모방한 Perceptron 
  ![](https://i.imgur.com/SHzBZr9.png)

퍼셉트론의 구성

1. 입력층(Input Layer): 입력값이 들어오는 층
2. 가중치(Weight): 입력값에 각각의 가중치가 곱해져서 합산된다
3. 합산(Sum): 가중치와 입력값의 곱을 모두 합산한다
4. 활성화 함수(Activation Function): 합산된 결과에 활성화 함수를 적용하여 출력을 생성한다. 활성함수로는 주로 계단 함수, 시그모이드 함수, ReLU(Rectified Linear Activation) 함수 등이 사용된다
5. 출력(Output): 활성화 함수를 거친 결과가 출력된다

<br>




### 다층 퍼셉트론 (MLP, Multi-Layer Perceptron)
- 입력층, 여러 개의 은닉층, 출력층으로 구성된 전방향 신경망. 퍼셉트론으로 구성되어 있다.

![](https://i.imgur.com/Bq7RAUw.png)

딥러닝 Layer의 구성

1. **입력층 (Input Layer):**
   - 모델의 첫 번째 층으로, 데이터를 입력받는 역할을 함.
   - 각 노드는 데이터의 특성을 나타내며, 입력 데이터의 차원에 따라 노드 개수 결정.
   - 데이터를 그대로 전달하거나 간단한 전처리만 수행.
2. **은닉층 (Hidden Layer):**
   - 입력층과 출력층 사이에 위치하며, 중간 표현을 학습하는 역할을 함.
   - 입력 데이터를 가중치와 활성화 함수를 통해 변환하여 추상화된 특징을 학습.
   - 여러 은닉층 사용 시 복잡한 패턴과 특징을 더 깊게 학습 가능.
   - 중간 표현은 직접 확인하기 어렵지만, 출력층으로 전달되어 최종 결과를 생성.
3. **출력층 (Output Layer):**
   - 모델의 최종 출력을 생성하는 층.
   - 문제에 따라 다양한 형태로 구성됨 (이진 분류, 다중 클래스 분류, 회귀 등).
   - 은닉층에서 학습된 중간 표현을 바탕으로 최종 결과 생성.

- 변형:
  - **Dropout MLP**: 과적합을 방지하기 위해 드롭아웃 정규화를 적용한 MLP.
  - **Batch Normalized MLP**: 배치 정규화를 통해 학습을 안정화하고 성능을 향상시킨 MLP.

  

<br>



### 순환 신경망 (RNN, Recurrent Neural Network)
- 시퀀스 데이터(순서가 있는 데이터. 시간에 따라 변화하는 주가 데이터, 문장에서 단어의 순서, 음성 신호 등)의 시간적 의존성을 모델링하는 신경망.
* 시간에 따른 데이터의 흐름과 변화를 예측하기 위해 만들어진 신경망
- 변형:
  - **LSTM (Long Short-Term Memory)**: 장기 의존성 문제를 해결하는 RNN의 변형.
  - **GRU (Gated Recurrent Unit)**: LSTM보다 간단한 구조로 비슷한 성능을 제공하는 RNN 변형.
  - **Bidirectional RNN**: 시퀀스를 정방향과 역방향으로 모두 학습하는 RNN.
  - **Attention Mechanism**: 특정 시점의 중요도를 학습하여 RNN의 성능을 향상시킴.
  ![](https://i.imgur.com/SWKOKUv.png)
*  `x`는 입력을, `h`는 은닉 상태를,`y`는 출력을 의미


![](https://i.imgur.com/vJ8C0jS.png)

![](https://i.imgur.com/TIq5MFa.png)

<br>

#### LSTM의 구조
1. **셀 상태 (Cell State) $C_{t}$:** LSTM의 핵심 메모리 부분으로, 과거의 정보를 보존하는 역할을 한다. 셀 상태는 각 타임 스텝(time step)에서 업데이트되며, 기억해야 하는 정보와 잊어야 하는 정보를 포함하고있다.

2. **입력 게이트 (Input Gate) $X_{t}$:** 입력 게이트는 현재 입력을 얼마나 반영할지를 결정한다. 

	2-1. 타임 스텝($x_{t}$)에서의 입력 벡터와 이전 타임 스텝($h_{t-1}$)에서의 은닉 상태(hidden state)를 결합한 결과를 만들고, 이 결과를 tanh 함수에 통과시켜 값의 범위를 -1에서 1 사이로 압축한다. 이 값은 새로운 후보 셀 상태(candidate cell state)다.
	
	2-2. 또 다른 입력 게이트에서 시그모이드 함수를 사용하여, 현재 입력이 얼마나 반영될지를 0과 1 사이의 값으로 결정한다. 1에 가까울수록 현재 입력을 많이 반영하게 된다. 이 값은 후보 셀 상태에 곱해져서 셀 상태를 업데이트하고 셀 상태에 더해진다.

<br>

3. **망각 게이트 (Forget Gate):** 망각 게이트는 셀 상태에서 어떤 정보를 잊어야 하는지를 결정한다. 시그모이드 함수를 사용하여 0과 1 사이의 값을 생성하며, 0에 가까울수록 해당 정보를 많이 잊게 된다. 

4. **출력 게이트 (Output Gate):** 출력 게이트는 셀 상태를 기반으로 현재 타임 스텝에서 어떤 값을 출력할지를 결정한다. 이 게이트는 시그모이드와 tanh 함수를 사용하여 값을 생성하고, 이 두 값을 곱한 값을 출력으로 사용한다.

5. **셀 상태 업데이트:** 입력 게이트를 통해 새로운 정보가 추가되고, 망각 게이트를 통해 일부 정보가 삭제된다. 이렇게 업데이트된 셀 상태는 출력 게이트를 통해 최종 출력으로 사용된다.

* 큰 노드 1개에 작은 노드 4개가 들어가있는 구조
* cell state -> 이 단어를 얼마나 기억해야할지 결정
* 잊어야 하는 값에는 음수를 곱하거나 작은값을 곱하기(x) 해서 빠르게 잊도록 한다.
* 기억해야 하는 값에는 시그모이드와, tanh를 곱한 뒤 더해준다(+)

<br>

### 컨볼루션 신경망 (CNN, Convolutional Neural Network)
- **설명**: 이미지나 시계열 데이터의 지역적 패턴을 학습하는 신경망.
- 변형:
  - **1D CNN**: 시계열 데이터 분석을 위한 1차원 컨볼루션.
  - **2D CNN**: 이미지 데이터 분석을 위한 2차원 컨볼루션.
  - **3D CNN**: 동영상 데이터 분석을 위한 3차원 컨볼루션.
  - **Dilated CNN**: 컨볼루션 필터의 수용 영역을 확장하여 더 넓은 범위의 패턴을 학습.
  - **LeNet, AlexNet, VGGNet, GoogLeNet (Inception Network), ResNet, DenseNet**: 주로 이미지 분류에 사용되는 다양한 CNN 구조.
  ![](https://i.imgur.com/FopH2HM.png)
#### CNN의 작동 과정
1. **입력 이미지**가 합성곱 계층에 전달
2. **합성곱 계층**에서 필터를 통해 이미지의 특징을 추출
3. **ReLU 활성화 함수**를 통해 비선형성을 추가
4. **풀링 계층**에서 데이터의 차원을 줄이고 중요한 특징을 강조
5. 이러한 과정을 여러 번 반복하여 더 고차원적이고 추상적인 특징을 추출
6. **플래트닝 (Flattening)** 후, **완전 연결 계층**에 데이터를 전달
	* Flattening : 다차원 배열(주로 2D 특징 맵)을 1차원 배열로 변환하는 과정
	* 이 과정은 완전 연결 계층(fully connected layer)에 데이터를 전달하기 위해 필요하다.
7. **출력 계층**에서 최종 분류나 예측을 수행


<br>

#### 필터(커널)의 작동 
* 필터는 일반적으로 3x3, 5x5등의 크기를 가진다
- **필터와 이미지의 합성곱**: 필터를 이미지의 좌측 상단에서 시작하여 우측 하단까지 이동하면서, 스탬프처럼 이미지를 필터의 크기로 찍어내며 필터의 각 요소와 해당 이미지 영역의 픽셀 값들을 곱한 후 합산한다.
* 이 합산 결과는 하나의 출력값이 되며, 이 값을 특징 맵(feature map)의 해당 위치에 기록한다.
![](https://i.imgur.com/mBYfbDu.gif)

![](https://i.imgur.com/LA7sXTt.png)

<br>

### 트랜스포머 (Transformer)
- 셀프 어텐션 메커니즘을 사용하여 시퀀스 데이터의 모든 위치 간의 관계를 모델링.
- 변형:
  - **TFT (Temporal Fusion Transformer)**: 시계열 데이터를 위한 트랜스포머 기반 모델.
  - **BERT (Bidirectional Encoder Representations from Transformers)**: 문맥을 양방향으로 학습하여 텍스트 분류 문제에 강력한 성능을 보임.
  - **GPT (Generative Pre-trained Transformer)**: 언어 모델링을 위한 트랜스포머 변형, 예측 및 분류 문제에도 사용 가능.
  - **RoBERTa (Robustly optimized BERT approach)**: BERT의 변형으로, 더 긴 학습과 큰 배치를 사용하여 성능을 향상.
  - **XLNet**: BERT와 GPT의 장점을 결합한 모델로, 더 좋은 성능을 보임.
  ![](https://i.imgur.com/NaOqFmt.png)


<br>



<br>

## 예측(회귀)에 특화된 모델
위의 공통 모델들 외에 예측(회귀)에 특화된 모델 

### 오토인코더 (Autoencoder)
- **설명**: 입력 데이터를 압축하고 재구성하는 비지도 학습 모델.
- 변형:
  - **Denoising Autoencoder**: 입력 데이터에 노이즈를 추가하고 이를 제거하는 방식으로 학습.
  - **Sparse Autoencoder**: 희소성을 부여하여 중요한 특징을 학습.
  - **Variational Autoencoder (VAE)**: 확률적 접근을 통해 데이터의 분포를 모델링.
  - **Sequence-to-Sequence Autoencoder**: 시퀀스 데이터를 입력과 출력으로 사용하는 변형.
  ![](https://i.imgur.com/ZOgBahw.png)



## 분류에 특화된 모델
위의 공통 모델들 외에 분류에 특화된 모델

### 앙상블 모델
- **설명**: 여러 모델의 예측 결과를 결합하여 최종 결과를 도출하는 방법.
- 변형:
  - **Voting Classifier**: 각 모델의 예측 결과를 투표로 결합.
  - **Bagging (Bootstrap Aggregating)**: 여러 모델을 병렬로 학습하여 평균화.
  - **Boosting**: 순차적으로 모델을 학습하여 성능을 향상.
    - **AdaBoost**: 잘못 예측된 데이터에 가중치를 부여하여 학습.
    - **Gradient Boosting**: 잔차를 줄이기 위해 모델을 순차적으로 학습.
    - **XGBoost**: Gradient Boosting의 효율성을 높인 모델.
  - **Stacking Classifier**: 여러 모델의 출력을 입력으로 사용하는 메타 모델을 결합.
  ![](https://i.imgur.com/i1xD8HH.png)
  ![](https://i.imgur.com/kruhzWz.png)
