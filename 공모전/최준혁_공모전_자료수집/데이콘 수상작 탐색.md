
## 1. 신용카드 사기 거래 탐지 AI 경진대회 월간 데이콘
* 자전거래(스스로 결제하고 결제대금을 받는것, 현금화)
* 도용
* 위조
###  (1위) 매우 간단한 1D AutoEncoder 활용한 신용카드 사기 거래 탐지 AI(Macro f1 score : 0.926)
* 이상 탐지 : 정상적인 데이터 패턴과는 다른 패턴을 가진 이상한 데이터를 식별하는 기술
[https://www.dacon.io/competitions/official/235930/codeshare/5508?page=1&dtype=recent](https://www.dacon.io/competitions/official/235930/codeshare/5508?page=1&dtype=recent)

1. **데이터 수집 및 전처리**
	- 데이터셋 : 정상, 사기 거래의 여부를 알 수 없는 신용 카드 데이터 (Unlabeled), 즉 **정답(label)이 없음**.
		* ID : 신용 카드 거래 ID
		* Column ('V1', 'V2', 'V3', ... ,'V30) : 비식별화된 신용 카드 거래 Feature
	- 데이터 전처리: 이상치 제거, 정규화 등 기본적인 전처리 수행

2. **사용된 모듈 및 라이브러리**
	- PyTorch : 딥러닝 프레임워크
		- 주요 기능: 동적 계산 그래프, GPU 가속, 다양한 신경망 모델 구현에 용이
		- 가이드: [PyTorch Tutorials](https://tutorials.pytorch.kr)
			```python
			import torch 
			import torch.nn as nn # 신경망 모듈(neural network modules), nn
			import torch.optim as optim # oprimizer, 모델의 학습 과정에서 손실함수를 최소화 하기 위한 가중치 조절 알고리즘 
			# 손실함수 : 모델이 학습하는 과정에서 모델의 예측과 실제 값 사이의 차이
			```
	
	- sklearn : 데이터 분석 패키지
		- 주요 기능: 데이터 전처리, 지도학습, 비지도학습 모델 제공
		- 가이드: [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
	- numpy : 수치 계산 패키지
		- 주요 기능: 다차원 배열 객체 제공, 수학적 함수 라이브러리
		- 가이드: [Numpy Documentation](https://numpy.org/doc/stable/user/quickstart.html)
	- pandas : 데이터 조작 및 분석 라이브러리
		- 주요 기능: 데이터프레임 자료구조, 데이터 정리 및 조작 기능 제공
		- 가이드: [Pandas Documentation](https://pandas.pydata.org/docs/)
	- tqdm : 프로그래스 바 라이브러리
		- 주요 기능: 루프 진행 상황을 시각적으로 표시
		- 76%|████████████████████████        | 7568/10000 [00:33<00:10, 229.00it/s]
		
3.  **모델 구성**
	- AutoEncoder : **비지도 학습(label이 없는 데이터)** 기반의 데이터 차원 축소 및 노이즈 제거 모델
		*  입력 데이터를 낮은 차원으로 압축변환한 후, 다시 원래의 데이터로 복원하는 방식으로 작동
		*  데이터의 차원 축소, 특징 추출, 잡음 제거 등에 사용
		- 구성: 인코더, 디코더
		- PyTorch를 이용한 모델 구현: `torch.nn.Module`을 상속받아 인코더와 디코더 네트워크를 정의


	1. **인코더(Encoder)**:
	    - 입력 데이터를 저차원 공간으로 압축합니다.
	    - 입력 데이터 $x$를 받아서 잠재 공간(latent space) 표현 $z$로 변환합니다.
	    - 일반적으로 여러 층의 신경망으로 구성되어 있으며, 차원을 줄이는 역할을 합니다.
	    - 인코더의 출력은 잠재 변수(latent variables) 또는 코드(code)라고 불립니다.
	2. **디코더(Decoder)**:
	    - 인코더에 의해 압축된 데이터를 다시 원래 차원으로 복원합니다.
	    - 잠재 공간 표현 $z$를 받아서 원래 입력 데이터와 유사한 출력 $\hat{x}$로 변환합니다.
	    - 인코더의 반대 역할을 하며, 입력 데이터를 복원하는 역할을 합니다.

 ![](https://i.imgur.com/fJ6NbSE.png)

4. **모델 학습 및 평가**
	- 학습 과정
		- 학습 데이터 및 검증 데이터 분할 방법: 훈련 데이터와 테스트 데이터로 분할
		- 학습 파라미터 설정: 학습률(LR), 배치 크기(BS), 에폭 수(EPOCHS) 등
		- 옵티마이저: Adam optimizer 사용
	- 평가 지표
		- 사용된 평가 지표: F1-score
		- 평가 결과 및 해석: 모델의 재현율과 정밀도를 고려한 F1-score로 성능 평가

```python
import torch
import torch.nn as nn

class AutoEncoder(nn.Module): 
    def __init__(self): 
        super(AutoEncoder, self).__init__()
        
        # 인코더 (Encoder) 부분
        # 이 데이터셋은 차원(변수)이 30으로 매우 작아서 Encoder에서 차원을 늘려주고 Decoder에서 다시 줄이는 형식으로 구현
        self.Encoder = nn.Sequential(
            nn.Linear(30, 64),         # 입력 차원 30을 64 차원으로 변환
            nn.BatchNorm1d(64),        # 배치 정규화 (훈련을 안정화하고 가속화)
            nn.LeakyReLU(),            # 활성화 함수로 LeakyReLU 사용
            nn.Linear(64, 128),        # 64 차원을 128 차원으로 변환
            nn.BatchNorm1d(128),       # 배치 정규화
            nn.LeakyReLU()             # 활성화 함수로 LeakyReLU 사용
        ) 
        # 디코더 (Decoder) 부분
        self.Decoder = nn.Sequential(
            nn.Linear(128, 64),        # 128 차원을 64 차원으로 변환
            nn.BatchNorm1d(64),        # 배치 정규화
            nn.LeakyReLU(),            # 활성화 함수로 LeakyReLU 사용
            nn.Linear(64, 30)          # 64 차원을 원래 입력 차원인 30으로 복원
        )
    def forward(self, x):
        # 인코더를 통해 입력 데이터를 잠재 공간으로 변환
        encoded = self.Encoder(x)
        # 디코더를 통해 잠재 공간 데이터를 원래 차원으로 복원
        decoded = self.Decoder(encoded)
        return decoded

# 사용 예시
# 모델 초기화
model = AutoEncoder()
# 임의의 입력 데이터 생성 (batch_size, input_dim) 형태
x = torch.randn(16, 30)  # batch size 16, input dimension 30
# 모델에 입력 데이터를 통과시켜 출력 얻기
output = model(x)
print(output.shape)  # torch.Size([16, 30])

```

<br>

## 2. 신용카드 사용자 연체 예측 AI 경진대회
* 어떤 범주에 속한 사람이 연체를 할 확률이 높은가?

### (1위) 팀 소회의실 Catboost(LogLoss 0.6581)
[https://www.dacon.io/competitions/official/235713/codeshare/2768?page=1&dtype=recent](https://www.dacon.io/competitions/official/235713/codeshare/2768?page=1&dtype=recent)

1. **데이터 수집 및 전처리**
    - 데이터셋 : 신용카드 사용자들의 개인 신상정보
	    -  성별, 자동차의 유무, 부동산 소유의 유무, 소득의 출처, 학력, 결혼 유무 등 범주형 데이터 타입
    - 데이터 전처리: 이상치 제거, 변수 생성 및 조작 등
2. **사용된 모듈 및 라이브러리**
    - Catboost : 그라디언트 부스팅 라이브러리
        - 주요 기능: 카테고리형 변수 처리를 자동화하여 모델 성능 향상
        - 가이드: [Catboost Documentation](https://catboost.ai/en/docs/)
    - category_encoders : 범주형 데이터 인코딩 라이브러리
        - 주요 기능: 다양한 범주형 변수 인코딩 방식 제공
        - 가이드: [Category Encoders Documentation](https://contrib.scikit-learn.org/category_encoders/)

   ![](https://i.imgur.com/WsTNtKF.png)
        ![](https://i.imgur.com/uedO26G.png)



    - sklearn : 데이터 분석 패키지
        - 주요 기능: 데이터 전처리, 지도학습, 비지도학습 모델 제공
        - 가이드: [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
    - numpy : 수치 계산 패키지
        - 주요 기능: 다차원 배열 객체 제공, 수학적 함수 라이브러리
        - 가이드: [Numpy Documentation](https://numpy.org/doc/stable/user/quickstart.html)
    - pandas : 데이터 조작 및 분석 라이브러리
        - 주요 기능: 데이터프레임 자료구조, 데이터 정리 및 조작 기능 제공
        - 가이드: [Pandas Documentation](https://pandas.pydata.org/docs/)
3. **모델 구성**
    - Catboost 모델 : 고성능 그라디언트 부스팅 모델
        - 특징: 카테고리형 변수를 자동으로 처리하여 성능 향상
        - 활용: 특정 대회에서 높은 성능을 기록한 모델


### Boosting
* 약한 학습기(학습 알고리즘)를 순차적으로 학습시켜 강력한 모델을 구축하는 방법
* 이전 학습기가 만든 오차에 집중하여 새로운 학습기를 학습시키는 방식
![](https://i.imgur.com/7H4pByO.png)

<br>

### Gradient Boosting
* 경사 하강법(Gradient Descent)을 이용하여 오차를 최소화하는 새로운 학습기
* 이전 학습기가 만든 잔여 오차(residual error)를 예측하는 새로운 모델을 만들어가며 점진적으로 학습을 진행
![](https://i.imgur.com/nM8araJ.png)
4. **모델 학습 및 평가**
    - 학습 과정
        - 학습 데이터 및 검증 데이터 분할 방법: 훈련 데이터와 테스트 데이터로 분할
        - 학습 파라미터 설정: 하이퍼파라미터 튜닝, 그리드 서치 등
    - 평가 지표
        - 사용된 평가 지표: 정확도, 정밀도, 재현율, F1-score 등
        - 평가 결과 및 해석: 모델의 다양한 성능 지표로 평가

```python	

from catboost import CatBoostClassifier
from sklearn.metrics import log_loss

# 모델 초기화
model_cat = CatBoostClassifier()

# 모델 학습
model_cat.fit(
    train_data,                  # 학습 데이터셋
    eval_set=valid_data,         # 검증 데이터셋
    use_best_model=True,         # 최적 모델 사용 여부 (기본적으로 True)
    early_stopping_rounds=100,   # 조기 종료(rounds) 설정 (100번 연속적으로 개선이 없을 경우)
    verbose=100                 # 학습 과정 로그 출력 주기 (100번마다 출력)
)
                
# 검증 데이터에 대한 예측 확률 계산
cat_pred[valid_idx] = model_cat.predict_proba(X_valid)
# 테스트 데이터에 대한 예측 확률 계산
cat_pred_test += model_cat.predict_proba(X_test) / n_fold
# 검증 데이터에 대한 로그 손실 계산 및 출력
print(f'CV Log Loss Score: {log_loss(y_valid, cat_pred[valid_idx]):.6f}')
# 전체 데이터에 대한 로그 손실 계산 및 출력
print(f'\tLog Loss: {log_loss(y, cat_pred):.6f}')

```


<br>



## 3. 2023 NH 투자증권 빅데이터 경진대회, “블룸버그, 나스닥과 함께 세계속으로!”

### (7위)Fin2Vec : Transformer 기반 종목 간 상관관계 분석 모델
[https://www.dacon.io/competitions/official/236145/codeshare/8902?page=1&dtype=random](https://www.dacon.io/competitions/official/236145/codeshare/8902?page=1&dtype=random)

1. **프로젝트 개요**
    - 프로젝트 목표: 나스닥 및 코스피 기업들 간의 관계성 기반으로 차트 재현 및 관계성 파악 모델 개발
    - 활동 요약:
        - 데이터 크롤링
        - 데이터 전처리 (Pandas, Numpy)   
        - Parallized Convolution Recurrent Network(PCRN) 모델 설계 및 구현
        - PCRN과 Transformer 융합하여 Fin2Vec 모델 설계 및 구현
        - Meta사의 Data2Vec 2.0 모델 차용 및 재설계
        - Fin2Vec를 통한 종목 클러스터링
        - Fin2Vec 활용 방안 제시
2. **데이터 수집 및 전처리**
    - 데이터셋: 2013년부터 2023년 상반기까지의 나스닥 및 코스피 종목 시세 데이터
    - 전처리: 데이터 크롤링, 정규화 및 토크나이징      
3. **사용된 모듈 및 라이브러리**
    - Transformer : 자연어 처리 모델
        - 주요 기능: self-attention 메커니즘을 통한 문맥 이해 및 관계성 학습
        - 활용: 종목 간 관계 분석을 위해 사용
    - PCRN (Parallized Convolution Recurrent Network) : 시계열 데이터 토크나이저
        - 주요 기능: 시계열 데이터를 압축하여 토큰화        
    - Meta Data2Vec 2.0 : 비지도 학습 모델
        - 주요 기능: 종목 간 관계성 학습에 활용    
    - Pandas : 데이터 조작 및 분석 라이브러리
        - 주요 기능: 데이터프레임 자료구조, 데이터 정리 및 조작 기능 제공    
        - 가이드: Pandas Documentation    
    - Numpy : 수치 계산 패키지
        - 주요 기능: 다차원 배열 객체 제공, 수학적 함수 라이브러리
        - 가이드: Numpy Documentation
4. **모델 구성**
    - Fin2Vec 모델 : Transformer와 PCRN을 융합한 모델
        - 구성: PCRN 인코더로 시계열 데이터 토크나이징, Transformer로 종목 간 관계성 학습
        - 활용: 종목 간 관계성 분석, 예측 및 군집화 작업
5. **모델 학습 및 평가**
    - 학습 과정:
        - 데이터 준비: 종목별 시세 데이터 수집 및 전처리
        - 모델 훈련: PCRN을 통한 토크나이징, Transformer로 관계성 학습
        - 하이퍼파라미터 설정: 학습률(LR), 배치 크기(BS), 에폭 수(EPOCHS) 등
    - 평가 지표:
        - 모델 성능 평가: 정확도, 정밀도, 재현율, F1-score 등

   
