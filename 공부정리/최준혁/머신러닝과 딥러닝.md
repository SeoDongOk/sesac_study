## 머신러닝과 딥러닝

**참고사항**

* 이 문서에서는 머신러닝과 딥러닝의 종류에 대한 내용만 다룹니다. 
* 트랜스포머에 관한 내용은 추후에 따로 작성하겠습니다.



<br>



### 알고리즘과 모델

* 알고리즘 : 컴퓨터가 어 떤일을 수행하는 명시적인 조건의 나열
* 모델 : 수학적인 기저가 있는 특정한 방법



<br>



### 알고리즘의 분류

수학적인 방법에 따라 4가지로 분류한다

1. **Similarity-Based Learning** : 유사성 기반 
   * 누구와 얼마나 비슷한가
   * 두 숫자가 있다면, 기하학적으로 두 숫자의 가까운 거리로 유사성을 측정
   * 예) K-최근접 이웃
2. **Probability-Based Learning** : 확률 기반
   * 데이터의 확률 분포를 모델링하여 예측
   * 새로운 데이터 포인트가 주어졌을때 그 확률을 계산하여 예측을 수행
   * 나이브 베이즈, 로지스틱 회귀
3. **Information-Based Learning** : 정보 기반
   * 정보 이론의 개념을 사용하여 모델을 학습
   * 주로 정보 이득(Information Gain)이나 엔트로피(Entropy)와 같은 척도를 사용하여 데이터를 분할
   * 스무고개와 비슷하다.
   * 예) 의사결정 나무
4. **Error-Based Learning** : 오차 기반
   * 틀린것을 측정해서 오차가 줄어들때 까지 공부하는것
   * 예) 딥러닝 



<br>



## 머신러닝(Machine Learning)

기계학습?

> 만약, 작업 T에 대해 기준 P로 측정한 성능이 경험 E로 인해 향상되었다면 그 프로그램은 작업 T에 대해 기준 P의 관점에서 경험 E로 부터 "배웠다"라고 말할 수 있다. - Tom Mitchell


![](https://i.imgur.com/F5DQlIY.png)



<br>



### 머신러닝의 분류

머신러닝은 일반적으로 다음 세 가지 주요 범주로 나뉜다.

![](https://i.imgur.com/f0F1MU3.png)

1. **Supervised Learning (지도학습)**
   - **정의**: **정답(라벨)이 있는 데이터셋**을 사용하여 모델을 학습시키는 방법.
   - **특징**: 입력 데이터와 해당 정답이 제공됨.
   - **예시**: 사진을 제시하고, 그것에 대한 답을 알려준다.

2. **Unsupervised Learning (비지도학습)**
   - **정의**: **정답(라벨)이 없는 데이터셋**을 사용하여 데이터의 구조나 패턴을 학습시키는 방법.
   - **특징**: 입력 데이터만 제공되고, 모델이 자체적으로 패턴을 찾아냄.
   - **예시**: 사진을 제시하고, 정답이 없는 데이터를 비슷한 특징끼리 군집화해서 새로운 데이터에 대한 결과를 예측한다.

3. **Reinforcement Learning (강화학습)**
   - **정의**: 환경과 상호작용하면서 경험을 통해 학습하는 방법.
   - **특징**: **데이터가 없음**. 보상을 통해 학습.
   - **예시**: 시도에 의한 경험에 의해서 학습한다.



<br>



### Supervised Learning (지도학습)

지도학습은 크게 두 가지 주요 하위 범주로 나뉜다.

* Regression (회귀)
* Classification (분류)

1. **Regression (회귀)**
   - **정의**: **연속적인 값**을 예측하는 방법.
   - **적용 사례**: 주가 예측, 온도 예측, 주택 가격 예측 등.
   - **주요 알고리즘**:
     - **Linear Regression (선형 회귀)**: 입력 변수와 출력 변수 간의 선형 관계를 모델링.
     - **Polynomial Regression (다항 회귀)**: 입력 변수와 출력 변수 간의 비선형 관계를 모델링.
     - **Decision Tree Regression (의사결정 나무 회귀)**: 데이터를 분할하여 예측을 수행.
     - **Support Vector Regression (SVR)**: 서포트 벡터 머신을 사용한 회귀.
     - **Random Forest Regression**: 다수의 의사결정 나무를 사용한 앙상블 회귀.
2. **Classification (분류)**
   - **정의**: **이산적인 클래스(범주)**로 데이터를 분류하는 방법.
   - **적용 사례**: 이메일 스팸 분류, 질병 진단, 이미지 분류 등.
   - **주요 알고리즘**:
     - **K-Nearest Neighbors (KNN)**: 가장 가까운 K개의 데이터 포인트를 사용하여 분류.
     - **Logistic Regression (로지스틱 회귀)**: 이진 분류를 위해 시그모이드 함수를 사용.
     - **Decision Tree Classification (의사결정 나무 분류)**: 데이터를 분할하여 분류를 수행.
     - **Support Vector Machine (SVM)**: 초평면을 사용하여 데이터를 분류.
     - **Random Forest Classification**: 다수의 의사결정 나무를 사용한 앙상블 분류.
     - **Naive Bayes (나이브 베이즈)**: 확률에 기반하여 데이터를 분류.
     - **Voting Classifier**:여러 다른 지도 학습 모델들의 예측을 조합하여 최종 예측을 수립하는 방법
     - **Bagging (Bootstrap Aggregating)**: 병렬로 여러 개의 독립적인 모델을 학습하고, 각 모델의 예측을 평균화하여 최종 예측을 만드는 방법
     - **Boosting**: 순차적으로 모델을 학습하여 이전 모델의 오류를 개선하는 방식
       * **AdaBoost**: 이전 모델에서 잘못 예측한 샘플에 가중치를 더 많이 주어 다음 모델을 학습한다.
       * **Gradient Boosting**: 이전 모델의 잔차(residual)를 줄이기 위해 다음 모델을 학습하는 방법.
       * **XGBoost**: Gradient Boosting을 효율적으로 구현한 모델로, 다양한 최적화 기법을 사용하여 성능을 향상
     - **Neural Networks (신경망)**: **딥러닝의 기본 단위**, 다층 퍼셉트론을 사용하여 복잡한 패턴을 학습.



<br>



### Unsupervised Learning (비지도 학습)

비지도학습은 크게 세 가지 주요 하위 범주로 나뉜다.

1. **Clustering (군집화)**

   * **정의**: 비슷한 데이터 포인트를 그룹으로 묶는 방법. 데이터의 구조를 파악하여 유사한 특성을 가진 데이터들을 하나의 군집으로 분류.

   * **적용 사례**:

     - 고객 세분화: 고객을 구매 패턴에 따라 그룹으로 분류.

     - 문서 분류: 비슷한 주제를 가진 문서들을 그룹으로 묶음.

     - 이미지 세그멘테이션: 이미지의 유사한 픽셀들을 그룹으로 묶음.


   * **주요 알고리즘**:

     - **K-means**: 데이터를 K개의 클러스터로 나눔.

     - **Hierarchical Clustering (계층적 군집화)**: 데이터의 계층적 구조를 파악하여 클러스터를 형성.

     - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: 밀도 기반의 클러스터링 방법으로, 노이즈와 이상치를 처리할 수 있음.

     - **Gaussian Mixture Models (GMM)**: 데이터가 여러 개의 가우시안 분포로 구성되어 있다고 가정하여 클러스터링.


2. **Dimensionality Reduction (차원 축소)**

   * **정의**: 고차원 데이터를 저차원으로 변환하여 데이터의 중요한 구조를 유지하면서 복잡성을 줄이는 방법.

   * **적용 사례**:

     - 데이터 시각화: 고차원 데이터를 2D 또는 3D로 변환하여 시각화.

     - 노이즈 제거: 불필요한 차원을 제거하여 데이터의 품질 향상.

     - 특징 추출: 중요한 특징만을 선택하여 데이터의 효율적 사용.


   * **주요 알고리즘**:

     - **PCA (Principal Component Analysis, 주성분 분석)**: 데이터의 분산을 최대화하는 방향으로 차원을 축소.

     - **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: 고차원 데이터를 저차원으로 시각화하는 데 주로 사용.

     - **LDA (Linear Discriminant Analysis, 선형 판별 분석)**: 클래스 간의 분산을 최대화하는 방향으로 차원을 축소 (사실 LDA는 지도학습에서 사용될 수 있으나, 차원 축소 기술로도 활용).


3. **Association Rule Learning (연관 규칙 학습)**

   * **정의**: 데이터 항목들 간의 흥미로운 관계를 발견하는 방법. 주로 대형 데이터베이스에서 자주 함께 나타나는 항목들을 찾음.

   * **적용 사례**:

     - 장바구니 분석: 어떤 상품들이 함께 구매되는지 파악하여 마케팅 전략 수립.

     - 의료 진단: 특정 증상과 질병 간의 관계 발견.

     - 추천 시스템: 사용자의 과거 행동을 기반으로 제품 추천.


   * **주요 알고리즘**:

     - **Apriori**: 빈번한 항목 집합을 찾아 연관 규칙을 생성.

     - **Eclat**: 항목 집합 간의 빈도를 계산하여 연관 규칙을 생성.

     - **FP-Growth (Frequent Pattern Growth)**: 빈번한 항목 집합을 트리 구조로 표현하여 연관 규칙을 생성.


**강화학습에 대한 내용은 다루지 않습니다.**



<br>



## 딥러닝(Deep Learning)

### 머신러닝 vs 딥러닝

![](https://i.imgur.com/5rtPD10.png)



<br>



#### 머신러닝 (Machine Learning)

머신러닝은 데이터 기반의 알고리즘을 사용하여 컴퓨터가 학습하고 패턴을 찾아내는 방법론이다. 주로 구조화된 데이터를 처리하며, 다음과 같은 주요 특징을 가진다.

- **알고리즘**: K-최근접 이웃, 의사결정 나무, 로지스틱 회귀, SVM 등 다양한 알고리즘 사용.
- **데이터 요구**: 레이블된(정답이 있는) 데이터가 필요한 경우가 많음.
- **특징**: 주로 지도학습과 비지도학습으로 나눠지며, 데이터의 특성을 학습하여 예측 및 분류를 수행.



<br>



#### 딥러닝 (Deep Learning)

딥러닝은 **머신러닝의 한 부분**으로, **인공 신경망(ANN)**을 기반으로 한 구조적인 학습 방법론이며, 다음과 같은 특징을 가진다.

- **알고리즘**: 다층 퍼셉트론(MLP), 컨볼루션 신경망 (CNN), 순환 신경망 (RNN), 트랜스포머 등 딥러닝 특화된 구조 사용.
- **데이터 요구**: 대규모의 레이블된 데이터셋 필요. 데이터의 특징 추출 및 변환 과정에서 자동으로 특성을 학습.
- **특징**: 복잡한 비선형 관계를 학습하고, 계층적 특성을 추출하여 높은 수준의 추상화를 수행.



<br>



### 인공 신경망(Artificial Neural Network)과 퍼셉트론(Perceptron)

* 인공 신경망은 생물학적 뉴런의 동작 원리에서 영감을 받아 발전한 모델이다.
* 인공신경망은 **딥러닝의 가장 기초적인 구조**이다.
* 인공신경망은 **퍼셉트론**이라는 기본적인 구성 요소들이 여러 층(layer)으로 쌓여서 구성된 네트워크다.

![](https://i.imgur.com/7DvsXog.png)

**뉴런의 작동 원리**:

- 뉴런은 신경세포로, 다른 뉴런과 시냅스를 통해 연결되어 있다.
- 입력 신호는 시냅스를 통해 받아들이며, 전기적 신호로 변환된다.
- 이 신호들은 축삭 돌기를 통해 전달되고, 축색 돌기의 끝에서 화학적 신호로 변환된다.
- 화학적 신호는 다시 다른 뉴런들로 이동한다.

  ![](https://i.imgur.com/SHzBZr9.png)

**퍼셉트론의 구성**:

1. 입력층(Input Layer): 입력값이 들어오는 층
2. 가중치(Weight): 입력값에 각각의 가중치가 곱해져서 합산된다
3. 합산(Sum): 가중치와 입력값의 곱을 모두 합산한다
4. 활성화 함수(Activation Function): 합산된 결과에 활성화 함수를 적용하여 출력을 생성한다. 활성함수로는 주로 계단 함수, 시그모이드 함수, ReLU(Rectified Linear Activation) 함수 등이 사용된다
5. 출력(Output): 활성화 함수를 거친 결과가 출력된다

**퍼셉트론의 작동 원리**:

- 퍼셉트론은 인공 신경망의 가장 간단한 형태로, **입력을 받아**(시냅스) **가중치와 함께 가중합을 계산**한다.
- 계산된 값은 **활성화 함수를 통해 출력으로 변환**된다(축삭 돌기).
- 활성화 함수는 일반적으로 임계값을 기준으로 이진 분류를 수행하며, 예를 들어 시그모이드 함수나 계단 함수가 사용된다.



<br>

## 딥러닝 지도학습 모델

* 아래 모델들의 **변형은 비지도 학습(정답이 없는 데이터)에도 이용이 가능하나, 지도학습에 관한 내용만 작성**했습니다.



### 다층 퍼셉트론 (MLP, Multi-Layer Perceptron)

* **인공 신경망(ANN, Artificial Neural Network)의 한 종류**
* **지도학습(회귀, 분류)에 주로 이용**된다.
* 입력층(Input), 여러 개의 은닉층(Hidden), 출력층(Output)으로 구성된 전방향 신경망. 퍼셉트론으로 구성되어 있다
* 여러 개의 은닉층을 포함하여 복잡한 비선형 관계를 모델링할 수 있다.

![](https://i.imgur.com/Bq7RAUw.png)

**MLP Layer의 구성**:

1. **입력층 (Input Layer):**
   - 모델의 첫 번째 층으로, 데이터를 입력받는 역할을 한다.
   - 각 노드는 데이터의 특성을 나타내며, 입력 데이터의 차원에 따라 노드 개수 결정한다.
   - 데이터를 그대로 전달하거나 간단한 전처리만 수행한다.
2. **은닉층 (Hidden Layer):**
   - 입력층과 출력층 사이에 위치하며, 중간 표현을 학습하는 역할을 한다.
   - 입력 데이터를 가중치와 활성화 함수를 통해 변환하여 추상화된 특징을 학습한다.
   - 여러 은닉층 사용 시 복잡한 패턴과 특징을 더 깊게 학습 가능하다.
   - 중간 표현은 직접 확인하기 어렵지만, 출력층으로 전달되어 최종 결과를 생성한다.
3. **출력층 (Output Layer):**
   - 모델의 최종 출력을 생성하는 층이다.
   - 문제에 따라 다양한 형태로 구성된다 (이진 분류, 다중 클래스 분류, 회귀 등).
   - 은닉층에서 학습된 중간 표현을 바탕으로 최종 결과를 생성한다.

- **변형**:
  - **Dropout MLP**: 과적합을 방지하기 위해 드롭아웃 정규화를 적용한 MLP.
  - **Batch Normalized MLP**: 배치 정규화를 통해 학습을 안정화하고 성능을 향상시킨 MLP.



<br>



### 순환 신경망 (RNN, Recurrent Neural Network)

* **인공 신경망(ANN, Artificial Neural Network)의 한 종류**
* **지도학습(회귀, 분류)에 주로 이용**된다.
* 시퀀스 데이터(순서가 있는 데이터. 시간에 따라 변화하는 주가 데이터, 문장에서 단어의 순서, 음성 신호 등)의 시간적 의존성을 모델링하는 신경망.
* 시간에 따른 데이터의 흐름과 변화를 예측하기 위해 만들어진 신경망

- **변형**:
  - **LSTM (Long Short-Term Memory)**: 장기 의존성 문제를 해결하는 RNN의 변형.
  - **GRU (Gated Recurrent Unit)**: LSTM보다 간단한 구조로 비슷한 성능을 제공하는 RNN 변형.
  - **Bidirectional RNN**: 시퀀스를 정방향과 역방향으로 모두 학습하는 RNN.
  - **Attention Mechanism**: 특정 시점의 중요도를 학습하여 RNN의 성능을 향상시킴.

  ![](https://i.imgur.com/SWKOKUv.png)

* `x`는 입력을, `h`는 은닉 상태를,`y`는 출력을 의미한다.

![](https://i.imgur.com/vJ8C0jS.png)

![](https://i.imgur.com/TIq5MFa.png)

#### LSTM의 구조
1. **셀 상태 (Cell State) $C_{t}$:** LSTM의 핵심 메모리 부분으로, 과거의 정보를 보존하는 역할을 한다. 셀 상태는 각 타임 스텝(time step)에서 업데이트되며, 기억해야 하는 정보와 잊어야 하는 정보를 포함하고있다.

2. **입력 게이트 (Input Gate) $X_{t}$:** 입력 게이트는 현재 입력을 얼마나 반영할지를 결정한다. 

	2-1. 타임 스텝($x_{t}$)에서의 입력 벡터와 이전 타임 스텝($h_{t-1}$)에서의 은닉 상태(hidden state)를 결합한 결과를 만들고, 이 결과를 tanh 함수에 통과시켜 값의 범위를 -1에서 1 사이로 압축한다. 이 값은 새로운 후보 셀 상태(candidate cell state)다.
	
	2-2. 또 다른 입력 게이트에서 시그모이드 함수를 사용하여, 현재 입력이 얼마나 반영될지를 0과 1 사이의 값으로 결정한다. 1에 가까울수록 현재 입력을 많이 반영하게 된다. 이 값은 후보 셀 상태에 곱해져서 셀 상태를 업데이트하고 셀 상태에 더해진다.

3. **망각 게이트 (Forget Gate):** 망각 게이트는 셀 상태에서 어떤 정보를 잊어야 하는지를 결정한다. 시그모이드 함수를 사용하여 0과 1 사이의 값을 생성하며, 0에 가까울수록 해당 정보를 많이 잊게 된다. 

4. **출력 게이트 (Output Gate):** 출력 게이트는 셀 상태를 기반으로 현재 타임 스텝에서 어떤 값을 출력할지를 결정한다. 이 게이트는 시그모이드와 tanh 함수를 사용하여 값을 생성하고, 이 두 값을 곱한 값을 출력으로 사용한다.

5. **셀 상태 업데이트:** 입력 게이트를 통해 새로운 정보가 추가되고, 망각 게이트를 통해 일부 정보가 삭제된다. 이렇게 업데이트된 셀 상태는 출력 게이트를 통해 최종 출력으로 사용된다.

* 큰 노드 1개에 작은 노드 4개가 들어가있는 구조
* cell state -> 이 단어를 얼마나 기억해야할지 결정
* 잊어야 하는 값에는 음수를 곱하거나 작은값을 곱하기(x) 해서 빠르게 잊도록 한다.
* 기억해야 하는 값에는 시그모이드와, tanh를 곱한 뒤 더해준다(+)



<br>



### 컨볼루션 신경망 (CNN, Convolutional Neural Network)

* **인공 신경망(ANN, Artificial Neural Network)의 한 종류**
* **지도학습(회귀, 분류)에 주로 이용**된다.
* 일반적인 MLP(Multi-Layer Perceptron)과는 구조적으로 다르다.
* 이미지 인식 및 패턴 인식을 위해 설계된 신경망으로, 입력 데이터의 공간적 구조를 보존하면서 효과적으로 학습할 수 있다.

- **변형**:
  - **1D CNN**: 시계열 데이터 분석을 위한 1차원 컨볼루션.
  - **2D CNN**: 이미지 데이터 분석을 위한 2차원 컨볼루션.
  - **3D CNN**: 동영상 데이터 분석을 위한 3차원 컨볼루션.
  - **Dilated CNN**: 컨볼루션 필터의 수용 영역을 확장하여 더 넓은 범위의 패턴을 학습.
  - **LeNet, AlexNet, VGGNet, GoogLeNet (Inception Network), ResNet, DenseNet**: 주로 이미지 분류에 사용되는 다양한 CNN 구조.

  ![](https://i.imgur.com/FopH2HM.png)

**CNN의 작동 과정**

1. **입력 이미지**가 합성곱 계층에 전달
2. **합성곱 계층**에서 필터를 통해 이미지의 특징을 추출
3. **ReLU 활성화 함수**를 통해 비선형성을 추가
4. **풀링 계층**에서 데이터의 차원을 줄이고 중요한 특징을 강조
5. 이러한 과정을 여러 번 반복하여 더 고차원적이고 추상적인 특징을 추출
6. **플래트닝 (Flattening)** 후, **완전 연결 계층**에 데이터를 전달
	* Flattening : 다차원 배열(주로 2D 특징 맵)을 1차원 배열로 변환하는 과정
	* 이 과정은 완전 연결 계층(fully connected layer)에 데이터를 전달하기 위해 필요하다.
7. **출력 계층**에서 최종 분류나 예측을 수행

**필터(커널)의 작동**

* 필터는 일반적으로 3x3, 5x5등의 크기를 가진다
* **필터와 이미지의 합성곱**: 필터를 이미지의 좌측 상단에서 시작하여 우측 하단까지 이동하면서, 스탬프처럼 이미지를 필터의 크기로 찍어내며 필터의 각 요소와 해당 이미지 영역의 픽셀 값들을 곱한 후 합산한다.
* 이 합산 결과는 하나의 출력값이 되며, 이 값을 특징 맵(feature map)의 해당 위치에 기록한다.
  ![](https://i.imgur.com/mBYfbDu.gif)

![](https://i.imgur.com/LA7sXTt.png)



<br>



## 딥러닝 비지도학습 모델

### 오토인코더 (Autoencoder)

* 입력 데이터를 인코딩하여 저차원 잠재 공간으로 매핑하고, 이를 다시 디코딩하여 입력 데이터를 재구성하는 신경망 구조
* **인공 신경망(ANN, Artificial Neural Network)의 한 종류**
* 입력 데이터에 대해 라벨이 없어도 학습할 수 있다
* **예측(회귀)**에 특화되어 있다.

**오토인코더의 구성**:

- **인코더(Encoder)**: 입력 데이터를 저차원의 잠재 벡터로 압축한다.
- **디코더(Decoder)**: 잠재 벡터를 원본 입력 데이터로 복원한다.

**용도**:

- **데이터의 특징 추출**: 잠재 공간에서의 표현을 통해 데이터의 중요한 특성을 추출할 수 있다.
- **데이터 압축**: 데이터의 차원을 줄이거나 노이즈를 제거하여 데이터를 정제할 수 있다.

**변형**:

- **Denoising Autoencoder**: 입력 데이터에 노이즈를 추가하고 이를 제거하는 방식으로 학습.
- **Sparse Autoencoder**: 희소성을 부여하여 중요한 특징을 학습.
- **Variational Autoencoder (VAE)**: 확률적 접근을 통해 데이터의 분포를 모델링.
- **Sequence-to-Sequence Autoencoder**: 시퀀스 데이터를 입력과 출력으로 사용하는 변형.

###   ![](https://i.imgur.com/ZOgBahw.png)



<br>



### 생성적 적대 신경망 (Generative Adversarial Network, GAN)

* **인공 신경망(ANN, Artificial Neural Network)의 한 종류**
* 생성자(Generator)와 판별자(Discriminator)라는 두 개의 신경망을 경쟁적으로 학습시키는 구조
* 두 개의 네트워크가 경쟁적으로 학습하여 실제처럼 보이는 데이터를 생성할 수 있다
* GAN은 **예측이나 분류와는 달리 데이터 생성을 목적**으로 하며, 이를 위해 생성자와 판별자 간의 경쟁적 학습을 이용한다.

**GAN의 구성**:

- **생성자(Generator)**: 랜덤 벡터를 입력으로 받아 실제 데이터와 비슷한 가짜 데이터를 생성한다.
- **판별자(Discriminator)**: 실제 데이터와 생성자가 생성한 가짜 데이터를 구분하는 분류기 역할을 한다.

**용도**:

* **이미지 생성 및 변환**: 새로운 얼굴 이미지를 생성하거나, 주어진 사진을 다른 스타일의 그림으로 변환할 수 있다.
* **영상 및 음성 생성**: 자율주행 자동차를 위한 가상의 도로 장면 생성 또는 음성 합성 등에 사용될 수 있다.
* **데이터 증강**: GAN은 기존 데이터셋을 확장하고 다양성을 추가하는 데 사용될 수 있다.
* **자연어 처리**: 특정 주제의 글을 생성하거나, 한 언어에서 다른 언어로의 번역을 개선하는 데 사용될 수 있다
* **보안 및 사기 탐지**: GAN은 이미지나 도면의 가짜를 판별하는 데 사용되기도 하며, 사기 탐지 시스템을 향상시키는 데 도움을 줄 수 있다.

![](https://i.imgur.com/75BokTk.png)